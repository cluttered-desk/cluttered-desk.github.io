<p>함수의 극점을 찾을 때는 도함수가 0이 되는 곳을 찾는다. 변수 한 개짜리일 때 $f^{‘}(x)=0$인 $x$를 찾는 거야 누구나 다 아는 사실이고, 이변수함수일 경우에는 $\frac{\partial f}{\partial x}=0$, 그리고 $\frac{\partial f}{\partial y}=0$일때, 즉 $\nabla f(x,y)=0$인 점들이 극점의 후보들이다. 변수 하나짜리 함수의 극점이 위로 볼록인지, 아래로 볼록인지 뭐 둘다 아닌지는 이계도함수를 이용했었다. 그리고 이변수인 경우에도 그런 툴이 있었는데. 바로 다음이다
 <script type="math/tex">% <![CDATA[
D=\begin{vmatrix}
 f_{xx}(x,y) & f_{xy}(x,y)\\
 f_{yx}(x,y) & f_{yy}(x,y)
 \end{vmatrix} %]]></script>
  대부분의 미적분학 교재에서 다루는 것처럼, $D&lt;0$이면 Saddle point고, $D&gt;0$인 경우에 $f_{xx}&gt;0$, $f_{xx}&lt;0$인 경우에 각각 local minima와 maxima이다. 이제 그 대략적인 이유를 살펴보자.
  사실 위의 matrix는 (determinent로만 써놓긴 했지만) Hessian이라 불리는 matrix이다. Hessian의 일반적인 정의는 다음과 같다.</p>

<p><strong>Definition 1: 함수 $f$가 $f:S\subset \mathbb{R} \rightarrow \mathbb{R}^n$이고 $f \in C^{2}$이면, $f$의 점 $\mathbf{x_0}\in U$에서의 Hessian은 다음과 같이 정의되는 quadratic function이다.</strong></p>

<script type="math/tex; mode=display">Hf(\mathbf{x_0})(\mathbf{h})=\frac{1}{2}\displaystyle\displaystyle\sum^{n}_{i,j=1}\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}h_i h_j</script>

<p>여기서 quadratic function이란 <script type="math/tex">f(\mathbf{h})=f(h_1,h_2,...,h_n)=\displaystyle\sum^{n}_{i,j=1}a_{ij}h_{i}h_{j}</script>
  꼴의 함수를 말한다.</p>

<p>이제 local maximum과 ‘strict’ local maximum에 대한 정의가 필요한데, local maximum이란, 익숙한 개념이지만 좀 더 엄밀하게 얘기하자면, $\lvert \mathbf{x_0}-\mathbf{x}\lvert &lt;\epsilon$인 모든 $x$에 대해, $f(\mathbf{x_0})\geq f(\mathbf{x})$인 경우에 $f$가 $\mathbf{x_0}$에서 local maximum을 가진다고 한다.</p>

<p>strict local maximum은 약간 다른데, $0&lt;\lvert \mathbf{x_0}-\mathbf{x}\lvert &lt;\epsilon$인 모든 $x$에 대해, (즉, $\mathbf{x} \neq \mathbf{x_0}$) $f(\mathbf{x_0})&gt; f(\mathbf{x})$인 경우에 $f$가 $\mathbf{x_0}$에서 strict local maximum을 가진다고 한다. 차이를 모르겠다면 $f(x,y)=-x^2$을 생각해 보자. $(0,0)$에서 local maximum을 가지지만, strict local maximum은 아니다. local minimum과 strict local minimum에 대해서는 굳이 따로 언급하지 않겠다. 그러니까 이제 저 위에 Second partial derivative test의 타당성을 증명하기 위해서는, Hessian의 어떤 점에서의 부호와 그 경우에 local minimum인지 maximum인지를 살펴보면 된다. 그 전에, 우선 lemma 하나를 증명하자.</p>

<p><strong>Lemma 1: 함수 $f:S\subset \mathbb{R} \rightarrow \mathbb{R}^n$이고 $f \in C^{3}$에 대해서, $f$를 다음과 같이 쓸 수 있고,</strong></p>

<p><strong><script type="math/tex">f(\mathbf{x_0}+\mathbf{h})=f(\mathbf{x_0})+\nabla f(\mathbf{x_0})\cdot\mathbf{h}+Hf(\mathbf{x_0})(\mathbf{h})+R_2(\mathbf{x_0},\mathbf{h})</script></strong></p>

<p><strong>$\displaystyle\lim_{\mathbf{h} \to 0} \frac{R_2(\mathbf{x_0},\mathbf{h})}{\lVert \mathbf{h}\lVert ^2}=0 $이다.</strong></p>

<p>증명은 너무 길어서 손으로 쓴다.
  <a href="https://www.flickr.com/gp/152463819@N08/6571vv"><img src="https://farm5.staticflickr.com/4443/37797555961_d430e96b95_b.jpg" alt="pf1" /></a>
  <a href="https://www.flickr.com/gp/152463819@N08/qcYw19"><img src="https://farm5.staticflickr.com/4497/37127374143_a1c1ebef2c_b.jpg" alt="pf2" /></a>
  <a href="https://www.flickr.com/gp/152463819@N08/poTSP0"><img src="https://farm5.staticflickr.com/4485/37797554811_f7b411f67f_b.jpg" alt="pf3" /></a>
  <a href="https://www.flickr.com/gp/152463819@N08/dcqBmY"><img src="https://farm5.staticflickr.com/4512/37749093466_f060598c41_b.jpg" alt="pf4" /></a>
  <a href="https://www.flickr.com/gp/152463819@N08/yb6b9h"><img src="https://farm5.staticflickr.com/4481/37127375343_34f98963c6_b.jpg" alt="pf5" /></a></p>

<p>이제 이걸 위에 다시 적용하면, $\mathbf{x_0}$가 $f$의 critical point이므로, $f(\mathbf{x_0}+\mathbf{h})-f(\mathbf{x_0})=Hf(\mathbf{x_0})(\mathbf{h})+R_2(\mathbf{h},\mathbf{x_0})$으로 쓸 수 있다. 이제 다음 정리를 증명하자.</p>

<p><strong>Theorem 1:함수 $f:S\subset \mathbb{R} \rightarrow \mathbb{R}^n$이고 $f \in C^{3}$에 대해서, $\mathbf{x_0} \in U$가 $f$의 critical point 일때, $Hf(\mathbf{x_0})$가 positive definite 라면 $f$가 strict local minimum을 가진다.</strong></p>

<p>증명: positive definite라는 이야기는 quadratic function $g(\mathbf{h})$가 $\mathbf{h}=0$일때 $0$, 그 외에는$g(\mathbf{h})&gt;0$인 경우를 말한다. 가정에 의해 $M\in \mathbb{R}$이고 $Hf(\mathbf{x_0})(\mathbf{h}) \geq \lVert \mathbf{h}\lVert ^2$인 $M$이 존재함을 알 수 있다. ($Hf(\mathbf{x_0})(\mathbf{h})$에서 $\lVert \mathbf{h}\lVert ^2$을 묶어 보자. 그러면 묶고 남은 것들은 정의역이 $\lVert \mathbf{h}\lVert ^2=1$짜리로 제한되므로 bounded가 된다.)</p>

<p>Lemma 1 에 의해 $ \displaystyle\lim_{\mathbf{h} \to 0} \frac{R_2(\mathbf{x_0},\mathbf{h})}{\lVert \mathbf{h}\lVert ^2}=0$이므로, $\epsilon \in \mathbb{R}^+$인 $\epsilon$에 대해 $\lvert \dfrac{R_2(\mathbf{x_0},\mathbf{h})}{\lVert \mathbf{h}\lVert ^2}\lvert &lt;\epsilon$이면 $\lVert \mathbf{h}\lVert &lt;\delta$인 $\delta \in \mathbb{R}^+$가 존재한다. $\epsilon=M\lVert h\lVert ^2$이라 하면 $\lVert \mathbf{h}\lVert &lt;\delta$일때</p>

<p><script type="math/tex">% <![CDATA[
-M\lVert h\lVert ^2<\dfrac{R_2(\mathbf{x_0},\mathbf{h})}{\lVert \mathbf{h}\lVert ^2}<M\lVert h\lVert ^2 %]]></script>이므로,</p>

<p><script type="math/tex">% <![CDATA[
0<Hf(\mathbf{x_0})(\mathbf{h})+R_2(\mathbf{h},\mathbf{x_0})=f(\mathbf{x_0}+\mathbf{h})-f(\mathbf{x_0}) %]]></script>이므로, $f$가 $\mathbf{x_0}$에서 strict local minimum을 가진다.</p>

<p>이제 같은 방식으로 $Hf(\mathbf{x_0})$가 negative definite라면 $f$가 $\mathbf{x_0}$에서 strict local maximum이라는 것 역시 알 수 있다. 이제 다시
 <script type="math/tex">% <![CDATA[
D=\begin{vmatrix}f_{xx}(x,y)&f_{xy}(x,y)\\f_{yx}(x,y)&f_{yy}(x,y)\end{vmatrix}​ %]]></script>
 로 돌아오자. $Hf(x,y)(\mathbf{h})=\dfrac{1}{2}(f_{xx}(x,y)h_1^2+2f_{xy}(x,y)h_{1}h_{2}+ f_{yy}(x,y)h_2^2)$이므로 완전제곱식으로 묶으면</p>

<p><script type="math/tex">\frac{1}{2}(f_{xx}(x,y)(h_1+\frac{f_{xy}(x,y)}{f_{xx}(x,y)}h_2)^2+ (f_{yy}(x,y)-\frac{f_{xy}(x,y)^2}{f_{xx}(x,y)})h_2^2)</script>이므로,</p>

<p>$det(D)&gt;0$인 경우에, $f_{xx}&gt;0$이면 positive definite, $f_{xx}&lt;0$이면 negative definite임을 쉽게 확인 가능하다. 이제 second partial derivative test의 local maximum과 local minimum인 조건을 얻은 셈이다. 유의해야 할 점은 $det(D)=0$인 경우에는 $(h_1+\dfrac{f_{xy}(x,y)}{f_{xx}(x,y)}h_2)=0$인 경우 때문에 positive definite 혹은 negative definite가 아니라는 점이다. $det(D)&gt;0$이라면 이 조건에서 $h_1=h_2$인 경우에만 $0$을 얻는다.</p>

<p>이제 문제는 $det(D)&lt;0$인 경우이다. 이때는 saddle point를 가진다. 이 문제에 대해서는 다음 글에서 다루겠다.</p>

